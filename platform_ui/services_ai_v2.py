from django.conf import settings
import logging
import time
import random
import json
import re

logger = logging.getLogger(__name__)

# Models to try in order
AVAILABLE_MODELS = [
    'gemini-2.0-flash',      # Latest fast model
    'gemini-1.5-flash',      # Standard fast model
    'gemini-1.5-pro',        # High intelligence fallback
]

def get_gemini_response(prompt):
    # Lazy Import & Config (New SDK)
    try:
        from google import genai
        from google.genai import types
        client = genai.Client(api_key=settings.GEMINI_API_KEY)
    except Exception as e:
        debug_msg = f"Import Error in V2: {e}"
        with open('ai_debug_v2.log', 'a', encoding='utf-8') as f:
             f.write(f"[{time.strftime('%H:%M:%S')}] {debug_msg}\n")
        logger.error(debug_msg)
        raise Exception(debug_msg)

    with open('ai_debug_v2.log', 'a', encoding='utf-8') as f:
         f.write(f"[{time.strftime('%H:%M:%S')}] Client initialized successfully. Prompting...\n")

    first_error = None
    
    # Retry configuration
    max_retries = 3
    base_delay = 1  # seconds

    for i, model_name in enumerate(AVAILABLE_MODELS):
        for attempt in range(max_retries + 1):
            try:
                # New SDK Usage
                # Enforce 15s timeout
                config = types.GenerateContentConfig(
                    # additional_params if needed, but timeout is often client-side in requests
                )
                
                # Note: The new SDK generates content via client.models
                response = client.models.generate_content(
                    model=model_name,
                    contents=prompt,
                    config=config
                )
                return response.text
            except Exception as e:
                # Check for 429 (Resource Exhausted)
                error_str = str(e)
                if "429" in error_str or "Resource exhausted" in error_str:
                    if attempt < max_retries:
                        delay = (base_delay * (2 ** attempt)) + random.uniform(0, 1)
                        logger.warning(f"Rate limit hit for {model_name}. Retrying in {delay:.2f}s... (Attempt {attempt+1}/{max_retries})")
                        time.sleep(delay)
                        continue
                
                # Non-retriable error or max retries reached for this model
                logger.warning(f"Failed with model {model_name}: {e}")
                if i == 0 and first_error is None:
                    first_error = e
                break # Move to next model
    
    # If all fail
    error_msg = str(first_error) if first_error else "Unknown error"
    logger.error(f"All Gemini models failed. Primary error: {error_msg}")
    
    # DEBUG: Write to file to ensure we see the error
    try:
        with open('ai_error.log', 'a', encoding='utf-8') as f:
            f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Connection Error: {error_msg}\n")
    except:
        pass
        
    return None # Return None to trigger Mock Fallback in caller

def get_mock_dashboard_data(revenue_today):
    """
    Returns realistic dummy data when AI is offline.
    """
    return {
        "prediction": int(revenue_today * 1.2),
        "analysis": "â˜ï¸ AI ì—°ê²° ëŒ€ê¸° ì¤‘ (ì˜¤í”„ë¼ì¸ ëª¨ë“œ): í˜„ìž¬ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¡œ ì¸í•´ AI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ê¸°ë³¸ ì˜ˆì¸¡ ëª¨ë¸ì´ ìž‘ë™ ì¤‘ìž…ë‹ˆë‹¤. ë‚ ì”¨ì™€ ë§¤ì¶œ ì¶”ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒìŠ¹ì„¸ê°€ ì˜ˆìƒë©ë‹ˆë‹¤.",
        "strategies": [
            { "category": "ì¸ë ¥", "icon": "ðŸ‘¥", "title": "í˜„ìž¥ ì¤‘ì‹¬ ìš´ì˜", "summary": "í”¼í¬íƒ€ìž„ ëŒ€ë¹„", "detail": "AI ì—°ê²°ì´ ì›í™œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‰ì†Œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì‹¬/ì €ë… í”¼í¬íƒ€ìž„ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”.", "score": 80 },
            { "category": "ìž¬ê³ ", "icon": "ðŸ“¦", "title": "í•„ìˆ˜ ìž¬ê³  ì ê²€", "summary": "ì£¼ìš” í’ˆëª© í™•ì¸", "detail": "ë„¤íŠ¸ì›Œí¬ ì´ìŠˆë¡œ ì‹¤ì‹œê°„ ìž¬ê³  ë¶„ì„ì´ ì§€ì—°ë˜ê³  ìžˆìŠµë‹ˆë‹¤. ìœ¡ì•ˆìœ¼ë¡œ ì£¼ìš” ìžìž¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.", "score": 85 },
            { "category": "ë§ˆì¼€íŒ…", "icon": "ðŸ“£", "title": "ë‹¨ê³¨ ê³ ê° ê´€ë¦¬", "summary": "ê¸°ì¡´ ì„œë¹„ìŠ¤ ìœ ì§€", "detail": "ë¬¸ìžë‚˜ SNSë¥¼ í†µí•´ ê¸ˆì¼ ì˜ì—… ì‹œê°„ì„ ì•ˆë‚´í•˜ê³  ë‹¨ê³¨ ê³ ê° ì´ë²¤íŠ¸ë¥¼ ì§„í–‰í•´ë³´ì„¸ìš”.", "score": 75 }
        ],
        "cheer_msg": "ë„¤íŠ¸ì›Œí¬ëŠ” ìž ì‹œ ì‰¬ì–´ê°€ë„, ì‚¬ìž¥ë‹˜ì˜ ì—´ì •ì€ ë©ˆì¶”ì§€ ì•ŠìŠµë‹ˆë‹¤! íž˜ë‚´ì„¸ìš”! ðŸ”¥"
    }

def generate_dashboard_analysis(sales_data, weather_data, inventory_data, event_data):
    """
    Combined AI Analyzer: Predicts revenue, analyzes flow, suggests strategy, and cheers.
    Everything in ONE call to speed up loading time (< 10s).
    """
    history_str = "\n".join(sales_data.get('history', []))
    revenue_today = sales_data.get('revenue', 0)
    
    prompt = f"""
    ë‹¹ì‹ ì€ 'Forkast AI'ì˜ ìˆ˜ì„ ë¶„ì„ê°€ìž…ë‹ˆë‹¤.
    ë‹¤ìŒ ë§¤ìž¥ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬, 4ê°€ì§€ í•µì‹¬ ì •ë³´ë¥¼ **JSON í¬ë§·**ìœ¼ë¡œ í•œ ë²ˆì— ì¶œë ¥í•˜ì„¸ìš”.
    
    [ìž…ë ¥ ë°ì´í„°]
    1. 30ì¼ ë§¤ì¶œ ì¶”ì´:
    {history_str}
    
    2. ì˜¤ëŠ˜ í˜„í™©:
    - í˜„ìž¬ ë§¤ì¶œ: {revenue_today}ì›
    - ë‚ ì”¨: {weather_data}
    - ìž¬ê³ : {inventory_data}
    - ì´ë²¤íŠ¸: {event_data}
    
    [ë¶„ì„ í•µì‹¬ ìš”êµ¬ì‚¬í•­]
    - **ê°€ìž¥ ì¤‘ìš”:** '30ì¼ ë§¤ì¶œ ì¶”ì´' ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜¤ëŠ˜ì˜ 'ë‚ ì”¨'ì™€ 'ì´ë²¤íŠ¸'ê°€ ë§¤ì¶œì— ë¯¸ì¹  ì˜í–¥ì„ ë¶„ì„í•˜ì„¸ìš”. ë‹¨ìˆœížˆ í˜„ìž¬ ìˆ˜ì¹˜ë§Œ ë³´ì§€ ë§ê³ , ê³¼ê±° íŒ¨í„´(ìš”ì¼/ë‚ ì”¨ ë“±)ê³¼ ë¹„êµí•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
    
    [ìš”ì²­ ì‚¬í•­]
    ë‹¤ìŒ 4ê°€ì§€ í•„ë“œë¥¼ í¬í•¨í•œ JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.
    
    1. "prediction" (Number): ê³¼ê±° ì¶”ì„¸ì™€ ì˜¤ëŠ˜ ë³€ìˆ˜(ë‚ ì”¨/ì´ë²¤íŠ¸)ë¥¼ ì¢…í•©í•œ ìµœì¢… ì˜ˆìƒ ë§¤ì¶œ.
    2. "analysis" (String): ì‹¤ì‹œê°„ ë§¤ì¶œ íë¦„ ë¶„ì„ (í•œ ë¬¸ë‹¨). "ðŸ“ˆ ìƒìŠ¹ì„¸/í•˜ë½ì„¸: ë‚ ì”¨ì™€ ì´ë²¤íŠ¸ ì˜í–¥ìœ¼ë¡œ ~~~." í˜•ì‹.
    3. "strategies" (Array): ìµœì  ìš´ì˜ ì „ëžµ 3ê°€ì§€ (ì¸ë ¥/ìž¬ê³ /ë§ˆì¼€íŒ…).
        - ê° ê°ì²´: {{ "category": "ì¸ë ¥", "icon": "ðŸ‘¥", "title": "...", "summary": "...", "detail": "...", "score": 85 }}
    4. "cheer_msg" (String): ì‚¬ìž¥ë‹˜ì„ ìœ„í•œ ì§§ê³  ê°ì„±ì ì¸ ìœ„ë¡œ/ì‘ì› ë©”ì‹œì§€ (30ìž ì´ë‚´).
    
    [JSON ì¶œë ¥ ì˜ˆì‹œ - ì—„ê²© ì¤€ìˆ˜]
    {{
      "prediction": 1250000,
      "analysis": "ðŸ“ˆ ìƒìŠ¹ì„¸: ë§‘ì€ ë‚ ì”¨ë¡œ ìœ ë™ ì¸êµ¬ê°€ ëŠ˜ì–´ ì „ì£¼ ëŒ€ë¹„ 15% ìƒìŠ¹ íë¦„ìž…ë‹ˆë‹¤.",
      "strategies": [
        {{ "category": "ì¸ë ¥", "icon": "ðŸ‘¥", "title": "í”¼í¬íƒ€ìž„ ì§‘ì¤‘", "summary": "12ì‹œ~2ì‹œ ì•Œë°” ì¶”ê°€", "detail": "ì ì‹¬ í”¼í¬ê°€ ì˜ˆìƒë˜ë‹ˆ...", "score": 90 }},
        ...
      ],
      "cheer_msg": "ì‚¬ìž¥ë‹˜, ì˜¤ëŠ˜ ëŒ€ë°• ì¡°ì§ì´ ë³´ì—¬ìš”! íž˜ë‚´ì„¸ìš”! ðŸŒŸ"
    }}
    
    âš ï¸ ì˜¤ì§ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```json)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    """
    
    response_text = get_gemini_response(prompt)
    
    with open('ai_debug_v2.log', 'a', encoding='utf-8') as f:
         f.write(f"[{time.strftime('%H:%M:%S')}] AI Response: {str(response_text)[:100]}...\n")

    if response_text is None:
        logger.warning("Falling back to Mock Data due to AI connection failure.")
        return get_mock_dashboard_data(revenue_today)

    # JSON Parsing Logic
    try:
        # Clean potential markdown
        clean_text = re.sub(r'```json\s*|\s*```', '', response_text).strip()
        data = json.loads(clean_text)
        return data
    except Exception as e:
        logger.error(f"Failed to parse Unified AI JSON: {e}. Raw: {response_text}")
        return get_mock_dashboard_data(revenue_today)

# Legacy functions kept for individual testing if needed, or can be removed.
# For now, we keep them but they won't be used in the main flow.
def analyze_sales_flow(sales_data, weather_data):
    pass
def suggest_operational_strategy(sales_data, inventory_data, weather_data):
    pass
def get_emotional_care_message():
    pass
def predict_revenue_with_ai(history_data, weather_data, inventory_data, event_data):
    pass
def consult_ai(question):
    # Only this one is still used individually by the chat
    prompt = f"""
    ë‹¹ì‹ ì€ ìžì˜ì—…ìžë¥¼ ìœ„í•œ ì „ë¬¸ AI ë¹„ì„œìž…ë‹ˆë‹¤. (ë²•ë¥ , ë…¸ë¬´, ì„¸ë¬´, ë§ˆì¼€íŒ… ì§€ì‹ ë³´ìœ )
    ì§ˆë¬¸: "{question}"
    í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ë‹µë³€í•´ì¤˜ (ê³µì†í•œ í†¤ì•¤ë§¤ë„ˆ).
    """
    return get_gemini_response(prompt)
